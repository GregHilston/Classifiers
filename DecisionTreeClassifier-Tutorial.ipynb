{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Classifier\n",
    "In this exercise, we'll experiment with a decision tree classifier.  We'll explore the concept of over-fitting and how to prevent it.\n",
    "See [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)\n",
    "\n",
    "## Acknowledgements\n",
    "Much of this work draws on material originally presented by Joe Findlay to the Fort Collins Data Science Meetup in June 2018.  See https://github.com/findaz/FoCoAstronomy. Also the feature set used in the modeling came from Coursera [Data Driven Astronomy](https://www.coursera.org/learn/data-driven-astronomy/home/welcome)\n",
    "\n",
    "## Citation\n",
    "\n",
    "[Scikit-learn: Machine Learning in Python](http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up matplotlib and use a nicer set of plot parameters\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercises, we'll classify galactic objects based on their spectral characteristics.  We'll use data obtained from the [Sloan Digital Sky Survey](https://www.sdss.org/surveys/). The data has previously been downloaded into a `csv` file for convenience. Here's the Python code that produced the data set we'll be working with:\n",
    "\n",
    "```python\n",
    "from astroquery.sdss import SDSS\n",
    "\n",
    "# query quasars and galaxies\n",
    "NOBJECTS = 40000\n",
    "query_text = ('\\n'.join(\n",
    "    (\"SELECT TOP %i\" % NOBJECTS,\n",
    "    \"   p.objid, s.class as objtype, p.u, p.g, p.r, p.i, p.z, s.z as redshift, s.zerr as redshift_err\",\n",
    "    \"FROM PhotoObj AS p\",\n",
    "    \"   JOIN SpecObj AS s ON s.bestobjid = p.objid\",\n",
    "    \"WHERE \",\n",
    "    \"   p.u BETWEEN 0 AND 19.6\",\n",
    "    \"   AND p.g BETWEEN 0 AND 20\" ,\n",
    "    \"   AND (s.class = 'GALAXY' OR s.class = 'QSO' or s.class = 'STAR')\")))\n",
    "    \n",
    "res = SDSS.query_sql(query_text)\n",
    "\n",
    "df = res.to_pandas()\n",
    "\n",
    "```\n",
    "The resulting data frame, `df` was written to a `.csv` file called `star_data.csv` in the `data` folder of the current repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/star_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "We're going to build a classifier which predicts an object's `objtype` based on it's spectral characteristics `u`, `g`, `r`, `i` and `z`. What is the set of object types and how prevelant is each in the data set? Use something like:\n",
    "```python\n",
    "stars.groupby('objtype').count().objid\n",
    "```\n",
    "to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here to show counts by object type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see it visually, try this:\n",
    "```python\n",
    "stars.groupby('objtype').count()['objid'].plot.bar()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here to produce graph of counts by object type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = ['STAR', 'QSO', 'GALAXY']\n",
    "colors = ['red', 'green', 'blue']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the object types map out relative to the spectral measurements.\n",
    "Try this:\n",
    "```\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(12,4), sharey=True, sharex=True)\n",
    "for t,c,a in zip(types, colors,ax):\n",
    "    df = stars.query('objtype == @t')\n",
    "    a.scatter(df.u, df.g, color=c, label=t)\n",
    "    a.legend()\n",
    "plt.tight_layout()\n",
    "```\n",
    "and repeat for the  `(r,i)` and `(i,z)` spectral pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here to show plots of object type in the u,g plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here to show plots of object type in the r,i plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here to show plots of oject type in the i,z plane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some needed libraries\n",
    "from sklearn.model_selection import train_test_split # to partition the dataset into training and test\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix # for model evaluation\n",
    "from sklearn.tree import DecisionTreeClassifier # classifier model to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Coursera course cited above uses the __difference__ in adjacent spectral intensities as the features in its classification model.  Here we'll construct 4 features:\n",
    "1. U minus G\n",
    "1. G minus R\n",
    "1. R minus I\n",
    "1. I minus Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create features from input data set\n",
    "# returns (n,4) feature matrix and (n,) label vector\n",
    "def get_features_labels(df):\n",
    "    features = np.zeros((len(df), 4))\n",
    "    features[:,0] = df.u-df.g # U minus G\n",
    "    #your code here\n",
    "    features[:,1] = None # G minus R\n",
    "    features[:,2] = None # R minus I\n",
    "    features[:,3] = None # I minus Z\n",
    "    \n",
    "    # get the object type labels\n",
    "    labels = None\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of all possible labels (need this later)\n",
    "labs = stars.objtype.unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function in [sklearn.model_selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection) to create an 80%/20% training and test split of the `stars` data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition data into training and test, 20% test\n",
    "trainingSet, testSet = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further want to partition the training set into a developement and validation set so that we can do model development and hyper-parameter selection without touching the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# further split training set into devel and validation, 20% validation\n",
    "devSet, valSet = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, get the features and labels from  each of the data sets: `dev`, `val` and `test`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the features and labels for each of the sets\n",
    "x_dev,  y_dev  = get_features_labels(devSet)\n",
    "x_val,  y_val  = None\n",
    "x_test, y_test = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier\n",
    "To use this technique, create a `DecisionTreeClassifier` object, as in:\n",
    "```python\n",
    "dtc = DecisionTreeClassifier()\n",
    "```\n",
    "then fit the model using the development set as argument to object's `.fit` method:\n",
    "```python\n",
    "dtc.fit(x_dev, y_dev)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = None\n",
    "\n",
    "#fit the model on the dev set\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy for both the developement and the validation test sets. The decision tree object's `predict` method will make predictions given a input feature matrix; `sklean.metric.accuracy_score` will compute accuracy from the true labels compared to the predicted labels, as in:\n",
    "```python\n",
    "# calculate prediction accuracy on the dev set\n",
    "pred_dev = dtc.predict(x_dev)\n",
    "acc_dev = accuracy_score(y_dev, pred_dev)\n",
    "```\n",
    "In the cell below, compute the accuracy on the training dataset and the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "# calculate prediction accuracy on the dev set\n",
    "pred_dev = None\n",
    "acc_dev = None\n",
    "\n",
    "#calculate prediction accuracy on the validation set\n",
    "pred_val = None\n",
    "acc_val = None\n",
    "\n",
    "print('Training Set Accuracy: ', acc_dev)\n",
    "print('Validation Set Accuracy: ', acc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn.metrics.confustion_matrix` will show counts of the actual labels versus the predicted labels for each of the labels in the data sets.  It takes two arguments, the first is the vector of true labels, the second is the vector of predicted labels.  Call it as follows:\n",
    "```python\n",
    "cm = pd.DataFrame(confusion_matrix(y_val, pred_val, labels=labs),\n",
    "                        index=labs, columns=labs)\n",
    "```\n",
    "Wrapping the function in a data frame as above puts the class labels on the result, making it easier to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "cm = None\n",
    "\n",
    "# display it\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Overfitting\n",
    "Decision Trees are notorious for overfitting. Left to its own devices, a decision tree estimation algorithm will build as complex a decision tree as it needs in order to fit all of the data.  Doing so makes it prone to prediction errors when the model encounters data that it hasn't seen before.\n",
    "\n",
    "In this section, we'll explore how the prediction accuracy on the development and validation data sets varies as a function of tree depth.  To do this, we need a function that will compute development and validation accuracy for a tree of a given depth.  The function should look something like the following:\n",
    "```python\n",
    "def dev_val_acc(x_dev, y_dev, x_val, y_val, max_depth):\n",
    "    \"\"\"\n",
    "    computes development and validation set accuracy for a decision tree of a specified maximum depth\n",
    "    \"\"\"\n",
    "    #create and fit the tree on the dev set\n",
    "    dtc = DecisionTreeClassifier(max_depth=max_depth)\n",
    "    dtc.fit(x_dev, y_dev)\n",
    "    \n",
    "    #compute dev set accuracy\n",
    "    pred_dev = dtc.predict(x_dev)\n",
    "    acc_dev = accuracy_score(pred_dev, y_dev)\n",
    "    \n",
    "    #compute validation set accuracy\n",
    "    pred_val = dtc.predict(x_val)\n",
    "    acc_val = accuracy_score(pred_val, y_val)\n",
    "    \n",
    "    #return the results\n",
    "    return (max_depth, acc_dev, acc_val)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "def dev_val_acc(x_dev, y_dev, x_val, y_val, max_depth):\n",
    "    \"\"\"\n",
    "    computes development and validation set accuracy for a decision tree of a specified maximum depth\n",
    "    \"\"\"\n",
    "    #create and fit the tree on the dev set\n",
    "    dtc = None\n",
    "    None\n",
    "    \n",
    "    #compute dev set accuracy\n",
    "    pred_dev = None\n",
    "    acc_dev = None\n",
    "    \n",
    "    #compute validation set accuracy\n",
    "    pred_val = None\n",
    "    acc_val = None\n",
    "    \n",
    "    #return the results\n",
    "    return (max_depth, acc_dev, acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test out on a tree 10-deep\n",
    "dev_val_acc(x_dev, y_dev, x_val, y_val, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the routine above on tree depths ranging from 1 to 30.  Store the result in a data frame for easy plotting later.  The code to do so should look someting like:\n",
    "```python\n",
    "model_score = pd.DataFrame([dev_val_acc(x_dev, y_dev, x_val, y_val,i) for i in range(1,30)],\n",
    "                          columns=['TreeDepth','DevAcc','ValAcc'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "model_score = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_score.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,4))\n",
    "\n",
    "\n",
    "ax1 = plt.subplot()\n",
    "\n",
    "ax1.set_title('Dev & Val Error Rate versus Tree Depth')\n",
    "\n",
    "ax1.plot(model_score.TreeDepth, 100.0*(1.0-model_score.DevAcc), linewidth=3, label = 'Development')\n",
    "ax1.plot(model_score.TreeDepth, 100.0*(1.0-model_score.ValAcc), linewidth=3, label = 'Validation')\n",
    "ax1.set_ylabel('Error Rate %')\n",
    "ax1.set_xlabel('Tree Depth')\n",
    "ax1.grid()\n",
    "\n",
    "ax1.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above is a classic illustration of over fitting.  In the figure, the error rate decreases continuously until it reaches zero, where as the error rate on the validation set drops for a bit, but then begins to increase as the model gets more complex (the decision tree gets deeper).\n",
    "\n",
    "The model with the least __validation__ error is the model we want to go with. Here we determine at what tree depth the least validation error occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index in the data frame of the highest validation set accuracy:\n",
    "model_score.ValAcc.values.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree depth of the highest validation set accuracy:\n",
    "max_depth = model_score.TreeDepth[ model_score.ValAcc.values.argmax() ]\n",
    "print('Tree depth of Highest Validation Set Accuracy: %d' % max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set Accuracy\n",
    "Now we know the value of the max_depth hyperparameter for our model, let's see how accurate we are on the test data set: data which we haven't encountered until now.\n",
    "\n",
    "First, reestimate the model on the combined development and validation data sets. To do so, stack them on top of each other, for both the feature matrix and the label vector. `np.vstack` is the tool for the job here, as in:\n",
    "```python\n",
    "x_train = np.vstack([x_dev, x_val])\n",
    "y_train = np.vstack([y_dev.values.reshape(-1,1), y_val.values.reshape(-1,1)])\n",
    "```\n",
    "The reshaping of the y values is necessary because the `get_features_label` function above returns the y labels as a `Pandas.Series` as opposed to an `ndarray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "# estimate the model on the combined dev and val sets\n",
    "x_train = None\n",
    "y_train = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create and fit the model on the combined data set and use the max_depth parameter, as in:\n",
    "```python\n",
    "dtc_md = DecisionTreeClassifier(max_depth=max_depth)\n",
    "dtc_md.fit(x_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_md = None #use max_depth computed above\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy for both the developement and the test sets. The decision tree object's `predict` method will make predictions given a input feature matrix; `sklean.metric.accuracy_score` will compute accuracy from the true labels compared to the predicted labels, as in:\n",
    "```python\n",
    "# calculate prediction accuracy on the training set\n",
    "pred_train_md = dtc_md.predict(x_train)\n",
    "acc_train_md = accuracy_score(y_train, pred_train_md)\n",
    "```\n",
    "In the cell below, compute the accuracy on the training dataset and the test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "# calculate prediction accuracy on the training set\n",
    "pred_train_md = None\n",
    "acc_train_md = None\n",
    "\n",
    "#calculate prediction accuracy on the test set\n",
    "pred_test_md = None\n",
    "acc_test_md = None\n",
    "\n",
    "print('Training Set Accuracy: ', acc_train_md)\n",
    "print('Test Set Accuracy: ', acc_test_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn.metrics.confustion_matrix` will show counts of the actual labels versus the predicted labels for each of the labels in the data sets.  It takes two arguments, the first is the vector of true labels, the second is the vector of predicted labels.  Call it as follows:\n",
    "```python\n",
    "cm_md = pd.DataFrame(confusion_matrix(y_test, pred_test_md, labels=labs),\n",
    "                        index=labs, columns=labs)\n",
    "```\n",
    "Wrapping the function in a data frame as above puts the class labels on the result, making it easier to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "cm_md = None\n",
    "\n",
    "# display it\n",
    "cm_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "Let's compare how the unconstrained (overfitting) model and the constrained (max_depth) model perform on the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the accuracy on the test set of the unconstrained model\n",
    "pred_test = dtc.predict(x_test)\n",
    "acc_test = accuracy_score(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a data frame for tabluar comparison of results\n",
    "mod_res=pd.DataFrame([[acc_dev, acc_test], [acc_train_md, acc_test_md]],\n",
    "                    columns=['Development','Test'],\n",
    "                    index=['Unconstrained','MaxDepth='+str(max_depth)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show graphically\n",
    "ax=mod_res.plot.bar(title='Model Accuracy')\n",
    "ax.legend(loc='lower right', title='Data Set')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.grid()\n",
    "ax.set_xlabel('Model Type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Over-fitting leads to overstating the accuracy of the classifier.  Constraining the model to counteract the overfitting leads to a better estimate of how well the model will do when presented with data it hasn't seen before.\n",
    "\n",
    "Hopefully you've learned how to construct a decision tree classifier, detect overfitting and correct for its ill effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
