{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Classifier\n",
    "In this exercise, we'll experiment with a multi-class logistic regression classifier.  We'll compare the performance of a one-versus-all variant to that of a multinomial variant.\n",
    "See [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)\n",
    "\n",
    "## Acknowledgements\n",
    "Much of this work draws on material originally presented by Joe Findlay to the Fort Collins Data Science Meetup in June 2018.  See https://github.com/findaz/FoCoAstronomy. Also the feature set used in the modeling came from Coursera [Data Driven Astronomy](https://www.coursera.org/learn/data-driven-astronomy/home/welcome)\n",
    "\n",
    "## Citation\n",
    "\n",
    "[Scikit-learn: Machine Learning in Python](http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up matplotlib and use a nicer set of plot parameters\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercises, we'll classify galactic objects based on their spectral characteristics.  We'll use data obtained from the [Sloan Digital Sky Survey](https://www.sdss.org/surveys/). The data has previously been downloaded into a `csv` file for convenience. Here's the Python code that produced the data set we'll be working with:\n",
    "\n",
    "```python\n",
    "from astroquery.sdss import SDSS\n",
    "\n",
    "# query quasars and galaxies\n",
    "NOBJECTS = 40000\n",
    "query_text = ('\\n'.join(\n",
    "    (\"SELECT TOP %i\" % NOBJECTS,\n",
    "    \"   p.objid, s.class as objtype, p.u, p.g, p.r, p.i, p.z, s.z as redshift, s.zerr as redshift_err\",\n",
    "    \"FROM PhotoObj AS p\",\n",
    "    \"   JOIN SpecObj AS s ON s.bestobjid = p.objid\",\n",
    "    \"WHERE \",\n",
    "    \"   p.u BETWEEN 0 AND 19.6\",\n",
    "    \"   AND p.g BETWEEN 0 AND 20\" ,\n",
    "    \"   AND (s.class = 'GALAXY' OR s.class = 'QSO' or s.class = 'STAR')\")))\n",
    "    \n",
    "res = SDSS.query_sql(query_text)\n",
    "\n",
    "df = res.to_pandas()\n",
    "\n",
    "```\n",
    "The resulting data frame, `df` was written to a `.csv` file called `star_data.csv` in the `data` folder of the current repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/star_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "We're going to build a classifier which predicts an object's `objtype` based on it's spectral characteristics `u`, `g`, `r`, `i` and `z`. What is the set of object types and how prevelant is each in the data set? Use something like:\n",
    "```python\n",
    "stars.groupby('objtype').count().objid\n",
    "```\n",
    "to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here to show counts by object type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see it visually, try this:\n",
    "```python\n",
    "stars.groupby('objtype').count()['objid'].plot.bar()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here to produce graph of counts by object type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = ['STAR', 'QSO', 'GALAXY']\n",
    "colors = ['red', 'green', 'blue']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the object types map out relative to the spectral measurements.\n",
    "Try this:\n",
    "```\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(12,4), sharey=True, sharex=True)\n",
    "for t,c,a in zip(types, colors,ax):\n",
    "    df = stars.query('objtype == @t')\n",
    "    a.scatter(df.u, df.g, color=c, label=t)\n",
    "    a.legend()\n",
    "plt.tight_layout()\n",
    "```\n",
    "and repeat for the  `(r,i)` and `(i,z)` spectral pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here to show plots of object type in the u,g plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here to show plots of object type in the r,i plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here to show plots of oject type in the i,z plane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some needed libraries\n",
    "from sklearn.model_selection import train_test_split # to partition the dataset into training and test\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix # for model evaluation\n",
    "from sklearn.linear_model import LogisticRegression # classifier model to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Coursera course cited above uses the __difference__ in adjacent spectral intensities as the features in its classification model.  Here we'll construct 4 features:\n",
    "1. U minus G\n",
    "1. G minus R\n",
    "1. R minus I\n",
    "1. I minus Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create features from input data set\n",
    "# returns (n,4) feature matrix and (n,) label vector\n",
    "def get_features_labels(df):\n",
    "    features = np.zeros((len(df), 4))\n",
    "    features[:,0] = df.u-df.g\n",
    "    #your code here\n",
    "    features[:,1] = None # G minus R\n",
    "    features[:,2] = None # R minus I\n",
    "    features[:,3] = None # I minus Z\n",
    "    \n",
    "    # get the object type labels\n",
    "    labels = None\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of all possible labels (need this later)\n",
    "labs = stars.objtype.unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function in [sklearn.model_selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection) to create an 80%/20% training and test split of the `stars` data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition data into training and test, 20% test\n",
    "trainingSet, testSet = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further want to partition the training set into a developement and validation set so that we can do model development and hyper-parameter selection without touching the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# further split training set into devel and validation, 20% validation\n",
    "devSet, valSet = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, get the features and labels from  each of the data sets: `dev`, `val` and `test`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the features and labels for each of the sets\n",
    "x_dev,  y_dev  = get_features_labels(devSet)\n",
    "x_val,  y_val  = None\n",
    "x_test, y_test = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One versus Rest Classifier\n",
    "The One versus Rest classifier essentially reduces a classification with k labeles into k binary classifications, then picks the label with the highest probability. To use this technique, specify `ovr` as the value of the `multi_class` parameter of the logistic regression object, as in:\n",
    "```python\n",
    "clf_ovr = LogisticRegression(random_state=0, solver='liblinear', multi_class='ovr')\n",
    "```\n",
    "then fit the model using the development test set as argument to object's `.fit` method:\n",
    "```python\n",
    "clf_ovr.fit(x_dev, y_dev)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "clf_ovr = None\n",
    "\n",
    "#fit the model on the dev set\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy for both the developement and the validation test sets. The logistic regression object's `predict` method will make predictions given a input feature matrix; `sklean.metric.accuracy_score` will compute accuracy from the true labels compared to the predicted labels, as in:\n",
    "```python\n",
    "# calculate prediction accuracy on the dev set\n",
    "pred_ovr_dev = clf_ovr.predict(x_dev)\n",
    "acc_ovr_dev = accuracy_score(y_dev, pred_ovr_dev)\n",
    "```\n",
    "In the cell below, compute the accuracy on the training dataset and the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "# calculate prediction accuracy on the dev set\n",
    "pred_ovr_dev = None\n",
    "acc_ovr_dev = None\n",
    "\n",
    "#calculate prediction accuracy on the validation set\n",
    "pred_ovr_val = None\n",
    "acc_ovr_val = None\n",
    "\n",
    "print('OVR Training Set Accuracy: ', acc_ovr_dev)\n",
    "print('OVR Validation Set Accuracy: ', acc_ovr_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn.metrics.confustion_matrix` will show counts of the actual labels versus the predicted labels for each of the labels in the data sets.  It takes two arguments, the first is the vector of true labels, the second is the vector of predicted labels.  Call it as follows:\n",
    "```python\n",
    "cm_ovr = pd.DataFrame(confusion_matrix(y_val, pred_ovr_val, labels=labs),\n",
    "                        index=labs, columns=labs)\n",
    "```\n",
    "Wrapping the function in a data frame as above puts the class labels on the result, making it easier to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "cm_ovr = None\n",
    "\n",
    "#display it\n",
    "cm_ovr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Classifier\n",
    "The multinomial classifier models the  classification as a multinomial (what else?). To use this technique, specify `multinomial` as the value of the `multi_class` parameter of the logistic regression object, as in:\n",
    "```python\n",
    "clf_multi = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n",
    "```\n",
    "then fit the model using the development test set as argument to object's `.fit` method:\n",
    "```python\n",
    "clf_multi.fit(x_dev, y_dev)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "clf_multi = None\n",
    "\n",
    "#fit the model on the dev set\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy for both the developement and the validation test sets. The logistic regression object's `predict` method will make predictions given a input feature matrix; `sklean.metric.accuracy_score` will compute accuracy from the true labels compared to the predicted labels, as in:\n",
    "```python\n",
    "# calculate prediction accuracy on the dev set\n",
    "pred_multi_dev = clf_multi.predict(x_dev)\n",
    "acc_multi_dev = accuracy_score(y_dev, pred_ovr_dev)\n",
    "```\n",
    "In the cell below, compute the accuracy on the training dataset and the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "pred_multi_dev = None\n",
    "acc_multi_dev = None\n",
    "\n",
    "pred_multi_val = None\n",
    "acc_multi_val = None\n",
    "\n",
    "print('Multinomial Training Set Accuracy: ', acc_multi_dev)\n",
    "print('Multinomial Validation Set Accuracy: ', acc_multi_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn.metrics.confustion_matrix` will show counts of the actual labels versus the predicted labels for each of the labels in the data sets.  It takes two arguments, the first is the vector of true labels, the second is the vector of predicted labels.  Call it as follows:\n",
    "```python\n",
    "cm_multi = pd.DataFrame(confusion_matrix(y_val, pred_multi_val, labels=labs),\n",
    "                        index=labs, columns=labs)\n",
    "```\n",
    "Wrapping the function in a data frame as above puts the class labels on the result, making it easier to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "cm_multi = None\n",
    "\n",
    "#display the confusion matrix\n",
    "cm_multi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a data frame for tabluar comparison of results\n",
    "mod_res=pd.DataFrame([[acc_ovr_dev, acc_ovr_val], [acc_multi_dev, acc_multi_val]],\n",
    "                    columns=['Development','Validation'],\n",
    "                    index=['OneVersusAll','MultiNomial'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show graphically\n",
    "ax=mod_res.plot.bar(title='Model Accuracy')\n",
    "ax.legend(loc='lower right', title='Data Set')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.grid()\n",
    "ax.set_xlabel('Model Type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Not much difference between over all performance of the two model types, however the multinomial model does a better job of classifying QSOs.\n",
    "\n",
    "Hopefully you've learned how to construct different logistic regression models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
